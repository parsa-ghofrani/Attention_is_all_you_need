{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##\"Attention Is All You Need\"##"
      ],
      "metadata": {
        "id": "hbJgCqRwicRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "3EpdDRUDi3K_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.heads_dim = embed_size // heads\n",
        "\n",
        "        assert (self.heads_dim * heads == embed_size), \"embed_size must be divisible by heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.heads_dim, self.heads_dim, bias=False)\n",
        "        self.keys = nn.Linear(self.heads_dim, self.heads_dim, bias=False)\n",
        "        self.queries = nn.Linear(self.heads_dim, self.heads_dim, bias=False)\n",
        "        self.fc_out = nn.Linear(self.heads * self.heads_dim, embed_size, bias=False)\n",
        "\n",
        "    def forward(self, values, keys, query, mask):\n",
        "        N = query.shape[0]\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "        # split the embedding into self.heads pieces\n",
        "        values = values.reshape(N, value_len, self.heads, self.heads_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.heads_dim)\n",
        "        queries = query.reshape(N, query_len, self.heads, self.heads_dim)\n",
        "\n",
        "\n",
        "        values = self.values(values)\n",
        "        keys = self.keys(keys)\n",
        "        queries = self.queries(queries)\n",
        "\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "\n",
        "        # queries shape : (N, query_len, heads, heads_dim)\n",
        "        # keys shape : (N, key_len, heads, heads_dim)\n",
        "        # energy shape : (N, heads, query_len, key_len)\n",
        "        # it says for each word in our target (query_len)\n",
        "        # how much are we going to pay attention to each word in the source sentence (key_len)\n",
        "        # can be done using torch.bmm    here we used torch.einsum\n",
        "\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attention = torch.softmax(energy/ (self.embed_size)**(1/2), dim=3)\n",
        "\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
        "            N, query_len,self.heads*self.heads_dim\n",
        "        )\n",
        "\n",
        "        # attention shape : (N, heads, query_len, key_len)\n",
        "        # values shape : (N, values_len, heads, heads_dim)\n",
        "        # after einsum, multiplied to be : (N, query_len, heads, head_dims)\n",
        "        # then flatten last two dimensions\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "aUBXR3Setmz-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Head Self-Attention\n",
        "\n",
        "The below implementation corresponds to the **Multi-Head Self-Attention Mechanism** described in the paper *\"Attention Is All You Need\"* by Vaswani et al. Here's a breakdown of its components and their roles:\n",
        "\n",
        "## Key Concepts\n",
        "1. **Self-Attention Mechanism**:\n",
        "   The goal of self-attention is to compute a weighted sum of the values, where the weights are determined by the similarity between the query and keys.\n",
        "\n",
        "   \\[\n",
        "   \\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
        "   \\]\n",
        "\n",
        "   - \\(Q\\): Queries.\n",
        "   - \\(K\\): Keys.\n",
        "   - \\(V\\): Values.\n",
        "   - \\(d_k\\): Dimensionality of the key/query vectors.\n",
        "\n",
        "2. **Multi-Head Attention**:\n",
        "   Instead of performing a single attention computation, the mechanism is split into \\(h\\) attention heads to capture different aspects of the sequence.\n",
        "\n",
        "3. **Scaling**:\n",
        "   The dot-product of queries and keys is scaled by \\(\\sqrt{d_k}\\) to avoid large values that could lead to vanishing gradients in the softmax function.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## Code Explanation\n",
        "### Initialization\n",
        "- **`embed_size`**: Size of the input embeddings.\n",
        "- **`heads`**: Number of attention heads.\n",
        "- **`heads\\_dim`**: Dimensionality of each head, computed as:\n",
        "  \\[\n",
        "  \\text{heads\\_dim} = \\frac{\\text{embed\\_size}}{\\text{heads}}\n",
        "  \\]\n",
        "\n",
        "- **Linear Layers**:\n",
        "  - `self.values`, `self.keys`, and `self.queries` map input embeddings to their respective representations.\n",
        "  - `self.fc_out` combines the outputs of all heads into the original embedding dimension.\n",
        "\n",
        "### Forward Pass\n",
        "#### 1. Input Shapes\n",
        "- Input tensors \\(values\\), \\(keys\\), and \\(queries\\) typically have shapes:\n",
        "  \\[\n",
        "  (N, \\text{seq\\_len}, \\text{embed\\_size})\n",
        "  \\]\n",
        "  where \\(N\\) is the batch size, and \\(\\text{seq\\_len}\\) is the sequence length.\n",
        "\n",
        "#### 2. Splitting into Heads\n",
        "The embeddings are reshaped to split the embedding into multiple heads:\n",
        "\\[\n",
        "\\text{Shape after reshaping: } (N, \\text{seq\\_len}, \\text{heads}, \\text{heads\\_dim})\n",
        "\\]\n",
        "\n",
        "#### 3. Linear Transformations\n",
        "- Values (\\(V\\)), Keys (\\(K\\)), and Queries (\\(Q\\)) are transformed using their respective linear layers.\n",
        "\n",
        "#### 4. Energy Calculation\n",
        "The dot-product between queries and keys is computed using:\n",
        "\\[\n",
        "\\text{energy}[n, h, q, k] = Q[n, q, h, d] \\cdot K[n, k, h, d]\n",
        "\\]\n",
        "This is implemented as:\n",
        "\\[\n",
        "\\text{torch.einsum}(\"nqhd, nkhd \\to nhqk\")\n",
        "\\]\n",
        "\n",
        "The resulting shape of the energy is:\n",
        "\\[\n",
        "(N, \\text{heads}, \\text{query\\_len}, \\text{key\\_len})\n",
        "\\]\n",
        "\n",
        "#### 5. Masking (Optional)\n",
        "- If a mask is provided, positions with a mask value of \\(0\\) are set to \\(-\\infty\\):\n",
        "\\[\n",
        "\\text{energy}[i, j] = -\\infty \\quad \\text{if } \\text{mask}[i, j] = 0\n",
        "\\]\n",
        "\n",
        "#### 6. Attention Weights\n",
        "- Apply the softmax function across the \\(\\text{key\\_len}\\) dimension:\n",
        "\\[\n",
        "\\text{attention} = \\text{Softmax}\\left(\\frac{\\text{energy}}{\\sqrt{\\text{embed\\_size}}}\\right)\n",
        "\\]\n",
        "\n",
        "#### 7. Weighted Sum of Values\n",
        "Using the attention weights, compute a weighted sum of the values:\n",
        "\\[\n",
        "\\text{out}[n, q, h, d] = \\sum_{k} \\text{attention}[n, h, q, k] \\cdot \\text{values}[n, k, h, d]\n",
        "\\]\n",
        "\n",
        "The output shape becomes:\n",
        "\\[\n",
        "(N, \\text{query\\_len}, \\text{heads}, \\text{heads\\_dim})\n",
        "\\]\n",
        "\n",
        "#### 8. Combining Heads\n",
        "- The output is reshaped to combine the dimensions of `heads` and `heads_dim`:\n",
        "\\[\n",
        "(N, \\text{query\\_len}, \\text{embed\\_size})\n",
        "\\]\n",
        "\n",
        "#### 9. Final Linear Transformation\n",
        "- The combined output is passed through `self.fc_out` to project it back to the original embedding dimension.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "zDh3r50bjJNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion*embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion*embed_size, embed_size)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attention = self.attention(value,key,query,mask)\n",
        "        x = self.dropout(self.norm1(attention + query))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out"
      ],
      "metadata": {
        "id": "WAWyGnYBjtMy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#tranformer Block\n",
        "\n",
        "The above implementation corresponds to a single **Transformer Block**, combining **self-attention**, **residual connections**, and **position-wise feed-forward networks** with layer normalization and dropout. This block is a key part of both the encoder and decoder in the Transformer architecture.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Components\n",
        "1. **Multi-Head Self-Attention**:\n",
        "   The block begins with a self-attention mechanism, implemented using the `SelfAttention` class. This allows the model to compute dependencies between all positions in the sequence.\n",
        "\n",
        "2. **Residual Connections**:\n",
        "   Residual connections are added after both the self-attention and feed-forward layers to prevent vanishing gradients and to stabilize training:\n",
        "   \\[\n",
        "   \\text{output} = \\text{LayerNorm}(\\text{input} + \\text{layer\\_output})\n",
        "   \\]\n",
        "\n",
        "3. **Layer Normalization**:\n",
        "   Layer normalization normalizes the inputs across the embedding dimensions to improve convergence.\n",
        "\n",
        "4. **Feed-Forward Network (FFN)**:\n",
        "   A two-layer feed-forward network is applied to each position independently. The dimensions are expanded temporarily to increase model capacity:\n",
        "   \\[\n",
        "   \\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
        "   \\]\n",
        "\n",
        "5. **Dropout**:\n",
        "   Dropout is applied to the outputs of both the self-attention and the feed-forward layers to prevent overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "## Code Explanation\n",
        "### Initialization (`__init__`)\n",
        "- **`SelfAttention(embed_size, heads)`**: Computes multi-head self-attention.\n",
        "- **Layer Normalization**:\n",
        "  - `self.norm1`: Applied after the self-attention layer.\n",
        "  - `self.norm2`: Applied after the feed-forward layer.\n",
        "- **Feed-Forward Network**:\n",
        "  - Temporarily expands the embedding size by a factor of `forward_expansion` before projecting back to the original size.\n",
        "  - Structure:\n",
        "    \\[\n",
        "    \\text{FFN}(x) = \\text{Linear}(\\text{ReLU}(\\text{Linear}(x)))\n",
        "    \\]\n",
        "- **Dropout**:\n",
        "  - Regularization to prevent overfitting during training.\n",
        "\n",
        "---\n",
        "\n",
        "### Forward Pass\n",
        "#### 1. Self-Attention\n",
        "The block first computes self-attention using the `SelfAttention` module:\n",
        "\\[\n",
        "\\text{attention} = \\text{SelfAttention}(value, key, query, mask)\n",
        "\\]\n",
        "- **Input shapes**:\n",
        "  \\[\n",
        "  (N, \\text{seq\\_len}, \\text{embed\\_size})\n",
        "  \\]\n",
        "- **Output shape**:\n",
        "  \\[\n",
        "  (N, \\text{seq\\_len}, \\text{embed\\_size})\n",
        "  \\]\n",
        "\n",
        "#### 2. Add & Normalize (First Residual Connection)\n",
        "The result of self-attention is added to the original query (residual connection) and normalized:\n",
        "\\[\n",
        "x = \\text{LayerNorm}(\\text{attention} + \\text{query})\n",
        "\\]\n",
        "\n",
        "#### 3. Feed-Forward Network\n",
        "The normalized output passes through the feed-forward network:\n",
        "\\[\n",
        "\\text{forward} = \\text{FFN}(x)\n",
        "\\]\n",
        "\n",
        "#### 4. Add & Normalize (Second Residual Connection)\n",
        "The output of the feed-forward network is added to the input of the feed-forward layer and normalized:\n",
        "\\[\n",
        "\\text{out} = \\text{LayerNorm}(\\text{forward} + x)\n",
        "\\]\n",
        "\n",
        "#### 5. Dropout\n",
        "Dropout is applied after each residual connection to further regularize the model.\n",
        "\n",
        "---\n",
        "\n",
        "### Final Output\n",
        "The Transformer Block outputs the following:\n",
        "- **Output shape**:\n",
        "  \\[\n",
        "  (N, \\text{seq\\_len}, \\text{embed\\_size})\n",
        "  \\]\n",
        "\n",
        "This forms the processed representation of the input sequence, which can be passed to subsequent Transformer Blocks or other parts of the model.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary\n",
        "This implementation of the Transformer Block includes:\n",
        "- **Multi-Head Self-Attention** for capturing dependencies between sequence elements.\n",
        "- **Feed-Forward Networks** for per-position transformations.\n",
        "- **Residual Connections**, **Layer Normalization**, and **Dropout** for training stability and regularization.\n",
        "\n",
        "The Transformer Block is the foundation for building both the **encoder** and **decoder** in the Transformer architecture.\n"
      ],
      "metadata": {
        "id": "wMy-_qqZkpgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 src_vocab_size,\n",
        "                 embed_size,\n",
        "                 num_layers,\n",
        "                 heads,\n",
        "                 device,\n",
        "                 forward_expansion,\n",
        "                 dropout,\n",
        "                 max_length\n",
        "    ):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.device = device\n",
        "        self.word_embeddings = nn.Embedding(src_vocab_size, embed_size)\n",
        "        self.position_embeddings = nn.Embedding(max_length, embed_size)\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(embed_size, heads, dropout=dropout, forward_expansion=forward_expansion)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        N, seq_length = x.shape\n",
        "        positions = torch.arange(0, seq_length).expand(N,seq_length).to(self.device)\n",
        "\n",
        "        out = self.dropout(self.word_embeddings(x) + self.position_embeddings(positions))\n",
        "\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, out, out, mask)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "dhqP2zFgk1f7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder\n",
        "\n",
        "The Encoder is responsible for processing the input sequence into a representation that captures its contextual meaning, which can then be used by the Decoder. This implementation consists of embedding layers, positional encoding, and a stack of Transformer blocks.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Components\n",
        "1. **Input Embedding**:\n",
        "   Converts input tokens (word indices) into dense vector representations.\n",
        "\n",
        "2. **Positional Encoding**:\n",
        "   Adds positional information to the embeddings, enabling the model to capture sequential relationships.\n",
        "\n",
        "3. **Stack of Transformer Blocks**:\n",
        "   Multiple `TransformerBlock` layers are applied in sequence to process the embeddings. Each block combines:\n",
        "   - Multi-head self-attention.\n",
        "   - Feed-forward networks.\n",
        "   - Residual connections and normalization.\n",
        "\n",
        "4. **Dropout**:\n",
        "   Regularization to prevent overfitting during training.\n",
        "\n",
        "---\n",
        "\n",
        "## Code Explanation\n",
        "### Initialization (`__init__`)\n",
        "1. **Parameters**:\n",
        "   - `src_vocab_size`: Size of the source vocabulary.\n",
        "   - `embed_size`: Dimensionality of the embeddings.\n",
        "   - `num_layers`: Number of Transformer blocks in the encoder.\n",
        "   - `heads`: Number of attention heads in each block.\n",
        "   - `device`: Device for computation (e.g., CPU or GPU).\n",
        "   - `forward_expansion`: Expansion factor for the feed-forward layer in each Transformer block.\n",
        "   - `dropout`: Dropout rate for regularization.\n",
        "   - `max_length`: Maximum sequence length supported by the encoder.\n",
        "\n",
        "2. **Embedding Layers**:\n",
        "   - `self.word_embeddings`: Maps input tokens to dense vectors of size `embed_size`.\n",
        "   - `self.position_embeddings`: Maps positions to dense vectors of size `embed_size`.\n",
        "\n",
        "3. **Transformer Layers**:\n",
        "   - `self.layers`: A list of `num_layers` `TransformerBlock` modules.\n",
        "\n",
        "4. **Dropout**:\n",
        "   - Applied after adding embeddings and positional encodings.\n",
        "\n",
        "---\n",
        "\n",
        "### Forward Pass\n",
        "#### 1. Input Dimensions\n",
        "- Input tensor `x` has shape:\n",
        "  \\[\n",
        "  (N, \\text{seq\\_length})\n",
        "  \\]\n",
        "  where \\(N\\) is the batch size, and \\(\\text{seq\\_length}\\) is the length of the input sequence.\n",
        "\n",
        "#### 2. Positional Encoding\n",
        "- Compute positional indices for each token:\n",
        "  \\[\n",
        "  \\text{positions} = [0, 1, 2, \\ldots, \\text{seq\\_length}-1]\n",
        "  \\]\n",
        "  The shape of `positions` is:\n",
        "  \\[\n",
        "  (N, \\text{seq\\_length})\n",
        "  \\]\n",
        "\n",
        "#### 3. Combine Embeddings\n",
        "- Add the word embeddings and positional embeddings:\n",
        "  \\[\n",
        "  \\text{out} = \\text{Dropout}(\\text{word\\_embeddings}(x) + \\text{position\\_embeddings}(\\text{positions}))\n",
        "  \\]\n",
        "  The shape of `out` is:\n",
        "  \\[\n",
        "  (N, \\text{seq\\_length}, \\text{embed\\_size})\n",
        "  \\]\n",
        "\n",
        "#### 4. Pass Through Transformer Blocks\n",
        "- Pass `out` through each `TransformerBlock`:\n",
        "  \\[\n",
        "  \\text{out} = \\text{TransformerBlock}(\\text{out}, \\text{out}, \\text{out}, \\text{mask})\n",
        "  \\]\n",
        "  - The same tensor is used for `value`, `key`, and `query` in self-attention.\n",
        "  - The shape remains:\n",
        "    \\[\n",
        "    (N, \\text{seq\\_length}, \\text{embed\\_size})\n",
        "    \\]\n",
        "\n",
        "#### 5. Output\n",
        "- The final output represents the processed sequence, capturing its contextual information:\n",
        "  \\[\n",
        "  \\text{out} = (N, \\text{seq\\_length}, \\text{embed\\_size})\n",
        "  \\]\n",
        "\n",
        "---\n",
        "\n",
        "## Summary\n",
        "The Encoder processes input sequences as follows:\n",
        "1. Converts tokens into embeddings.\n",
        "2. Adds positional information.\n",
        "3. Applies a stack of `TransformerBlock`s to refine the representation.\n",
        "\n",
        "The Encoder's output is used by the Decoder for tasks like machine translation, text summarization, and more.\n"
      ],
      "metadata": {
        "id": "zhYdG3CTlEr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        self.norm = nn.LayerNorm(embed_size)\n",
        "        self.transformer_block = TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, value, key, src_mask, trg_mask):\n",
        "        attention = self.attention(x, x, x, trg_mask)\n",
        "        query = self.dropout(self.norm(attention + x))\n",
        "        out = self.transformer_block(value, key, query, src_mask)\n",
        "        return out"
      ],
      "metadata": {
        "id": "9K-Km0PiloyB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder Block\n",
        "\n",
        "The Decoder Block processes the target sequence, generating a contextual representation that incorporates both the target sequence and the source sequence information. This is achieved using self-attention, encoder-decoder attention, and feed-forward layers.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Components\n",
        "1. **Self-Attention**:\n",
        "   Allows the decoder to attend to the target sequence itself, ensuring that the decoder only \"sees\" the past tokens (up to the current position).\n",
        "\n",
        "2. **Residual Connections and Normalization**:\n",
        "   Residual connections are applied after both the self-attention and encoder-decoder attention layers, followed by layer normalization.\n",
        "\n",
        "3. **Encoder-Decoder Attention**:\n",
        "   This cross-attention mechanism allows the decoder to attend to the encoder's output, integrating information from the source sequence.\n",
        "\n",
        "4. **Transformer Block**:\n",
        "   Incorporates multi-head attention, feed-forward layers, residual connections, and normalization, similar to the encoder block.\n",
        "\n",
        "5. **Dropout**:\n",
        "   Regularization to prevent overfitting during training.\n",
        "\n",
        "---\n",
        "\n",
        "## Code Explanation\n",
        "### Initialization (`__init__`)\n",
        "1. **Parameters**:\n",
        "   - `embed_size`: Dimensionality of the embeddings.\n",
        "   - `heads`: Number of attention heads in multi-head attention.\n",
        "   - `forward_expansion`: Expansion factor for the feed-forward network.\n",
        "   - `dropout`: Dropout rate for regularization.\n",
        "   - `device`: Device for computation (e.g., CPU or GPU).\n",
        "\n",
        "2. **Components**:\n",
        "   - `self.attention`: Implements masked self-attention for the target sequence.\n",
        "   - `self.norm`: Layer normalization applied after self-attention.\n",
        "   - `self.transformer_block`: Processes the encoder-decoder attention and feed-forward steps.\n",
        "   - `self.dropout`: Applies dropout after residual connections.\n",
        "\n",
        "---\n",
        "\n",
        "### Forward Pass\n",
        "#### 1. Masked Self-Attention\n",
        "- The decoder attends to the target sequence using `SelfAttention`. The `trg_mask` ensures that the decoder can only attend to past tokens (and the current token), preventing it from \"cheating\" during training:\n",
        "  \\[\n",
        "  \\text{attention} = \\text{SelfAttention}(x, x, x, \\text{trg\\_mask})\n",
        "  \\]\n",
        "- The shape of `attention` is:\n",
        "  \\[\n",
        "  (N, \\text{trg\\_len}, \\text{embed\\_size})\n",
        "  \\]\n",
        "\n",
        "#### 2. Add & Normalize\n",
        "- A residual connection adds the input `x` to the output of self-attention, followed by dropout and layer normalization:\n",
        "  \\[\n",
        "  \\text{query} = \\text{LayerNorm}(\\text{attention} + x)\n",
        "  \\]\n",
        "\n",
        "#### 3. Encoder-Decoder Attention\n",
        "- The `TransformerBlock` uses the encoder's output (`value` and `key`) and the updated `query` to compute encoder-decoder attention. The `src_mask` ensures that irrelevant parts of the source sequence are ignored:\n",
        "  \\[\n",
        "  \\text{out} = \\text{TransformerBlock}(\\text{value}, \\text{key}, \\text{query}, \\text{src\\_mask})\n",
        "  \\]\n",
        "- The shape of `out` is:\n",
        "  \\[\n",
        "  (N, \\text{trg\\_len}, \\text{embed\\_size})\n",
        "  \\]\n",
        "\n",
        "---\n",
        "\n",
        "### Summary\n",
        "The Decoder Block performs the following:\n",
        "1. **Masked Self-Attention**:\n",
        "   Ensures the decoder focuses only on the tokens seen so far in the target sequence.\n",
        "\n",
        "2. **Encoder-Decoder Attention**:\n",
        "   Allows the decoder to attend to relevant parts of the source sequence output from the encoder.\n",
        "\n",
        "3. **Residual Connections, Normalization, and Dropout**:\n",
        "   Improve training stability and prevent overfitting.\n",
        "\n",
        "This structure is repeated in the full Transformer Decoder, with multiple decoder blocks stacked sequentially.\n",
        "\n",
        "---\n",
        "\n",
        "### Output\n",
        "- The output shape is:\n",
        "  \\[\n",
        "  (N, \\text{trg\\_len}, \\text{embed\\_size})\n",
        "  \\]\n",
        "- It represents the processed target sequence, incorporating information from both the target and source sequences.\n"
      ],
      "metadata": {
        "id": "2LlClHOLlxUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, trg_vocab_size, embed_size, num_layers, heads, forward_expansion,dropout, device,max_length):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.device = device\n",
        "        self.word_embeddings = nn.Embedding(trg_vocab_size, embed_size)\n",
        "        self.position_embeddings = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
        "        N, seq_length = x.shape\n",
        "        position = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "        x = self.dropout((self.word_embeddings(x)) + (self.position_embeddings(position)))\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
        "\n",
        "        out = self.fc_out(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "8_5WO34yl4_i"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Decoder\n",
        "\n",
        "The `Decoder` is designed to take the target sequence (partially constructed during training or from previous predictions during inference) and the encoder outputs to generate predictions. It consists of multiple stacked `DecoderBlock` layers.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Components\n",
        "\n",
        "1. **Word Embeddings**:\n",
        "   - Converts token indices in the target vocabulary into dense vector representations of size `embed_size`.\n",
        "\n",
        "2. **Positional Embeddings**:\n",
        "   - Encodes positional information for each token in the sequence to preserve order, as Transformers lack inherent sequential bias.\n",
        "\n",
        "3. **Decoder Blocks**:\n",
        "   - Each `DecoderBlock` processes the current representation of the target sequence while incorporating the encoded source sequence.\n",
        "\n",
        "4. **Dropout**:\n",
        "   - Helps regularize the model by preventing overfitting.\n",
        "\n",
        "5. **Final Linear Layer**:\n",
        "   - Maps the `embed_size` vectors to the size of the target vocabulary (`trg_vocab_size`) to predict probabilities for each possible token.\n",
        "\n",
        "---\n",
        "\n",
        "## Code Walkthrough\n",
        "\n",
        "### Initialization (`__init__`)\n",
        "1. **Parameters**:\n",
        "   - `trg_vocab_size`: Size of the target vocabulary.\n",
        "   - `embed_size`: Dimensionality of token embeddings.\n",
        "   - `num_layers`: Number of stacked decoder blocks.\n",
        "   - `heads`: Number of attention heads in multi-head attention.\n",
        "   - `forward_expansion`: Expansion factor for the feed-forward network inside each block.\n",
        "   - `dropout`: Dropout rate for regularization.\n",
        "   - `device`: The computation device (e.g., CPU or GPU).\n",
        "   - `max_length`: Maximum length of input sequences.\n",
        "\n",
        "2. **Components**:\n",
        "   - `word_embeddings`: Maps target vocabulary indices to dense vectors.\n",
        "   - `position_embeddings`: Encodes positional information for tokens.\n",
        "   - `layers`: A stack of `DecoderBlock` instances.\n",
        "   - `fc_out`: Maps the decoder's final outputs to the target vocabulary space.\n",
        "   - `dropout`: Applies dropout to embeddings and residual connections.\n",
        "\n",
        "---\n",
        "\n",
        "### Forward Pass\n",
        "#### Inputs:\n",
        "- `x`: Target sequence (as token indices) of shape \\((N, \\text{trg\\_len})\\).\n",
        "- `enc_out`: Output of the encoder, which represents the source sequence context.\n",
        "- `src_mask`: Source sequence mask to ignore padding tokens.\n",
        "- `trg_mask`: Target sequence mask to prevent the decoder from attending to future tokens.\n",
        "\n",
        "#### 1. Embedding and Positional Encoding:\n",
        "\\[\n",
        "\\text{x\\_embed} = \\text{word\\_embeddings}(x)\n",
        "\\]\n",
        "\\[\n",
        "\\text{pos\\_embed} = \\text{position\\_embeddings}(\\text{position})\n",
        "\\]\n",
        "\\[\n",
        "x = \\text{Dropout}(\\text{x\\_embed} + \\text{pos\\_embed})\n",
        "\\]\n",
        "\n",
        "Here:\n",
        "- `x_embed` captures the semantic representation of the target tokens.\n",
        "- `pos_embed` encodes the token positions.\n",
        "- Shape of `x`: \\((N, \\text{trg\\_len}, \\text{embed\\_size})\\).\n",
        "\n",
        "#### 2. Decoder Blocks:\n",
        "Each `DecoderBlock` processes the sequence and integrates source context:\n",
        "\\[\n",
        "x = \\text{DecoderBlock}(x, \\text{enc\\_out}, \\text{enc\\_out}, \\text{src\\_mask}, \\text{trg\\_mask})\n",
        "\\]\n",
        "This is repeated for all layers in `self.layers`.\n",
        "\n",
        "#### 3. Final Linear Layer:\n",
        "The final layer projects the output into the target vocabulary space:\n",
        "\\[\n",
        "\\text{out} = \\text{fc\\_out}(x)\n",
        "\\]\n",
        "- Shape of `out`: \\((N, \\text{trg\\_len}, \\text{trg\\_vocab\\_size})\\).\n",
        "\n",
        "---\n",
        "\n",
        "## Output:\n",
        "- `out`: Predicted token probabilities for each position in the target sequence.\n",
        "- Shape: \\((N, \\text{trg\\_len}, \\text{trg\\_vocab\\_size})\\).\n",
        "\n",
        "---\n",
        "\n",
        "## Summary of Functionality:\n",
        "1. **Embeddings**:\n",
        "   Combine semantic and positional embeddings for the target sequence.\n",
        "\n",
        "2. **Masked Self-Attention**:\n",
        "   Prevents information leakage by ensuring the decoder can only attend to already-seen tokens.\n",
        "\n",
        "3. **Encoder-Decoder Attention**:\n",
        "   Incorporates context from the encoder output, allowing the decoder to relate target tokens to the source sequence.\n",
        "\n",
        "4. **Prediction**:\n",
        "   Produces token probabilities for the target sequence.\n",
        "\n",
        "This class forms the backbone of the decoder side in sequence-to-sequence tasks, such as machine translation and text generation.\n"
      ],
      "metadata": {
        "id": "9AvV_fUamEPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx,\n",
        "                 embed_size=256, num_layers=6, forward_expansion=4,\n",
        "                 heads=8, dropout=0.1, device='cpu',max_length=100):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder(src_vocab_size,embed_size, num_layers,\n",
        "                               heads,device,forward_expansion,dropout,max_length)\n",
        "\n",
        "        self.decoder = Decoder(trg_vocab_size,embed_size,num_layers,\n",
        "                               heads,forward_expansion,dropout,device,max_length)\n",
        "\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        # (N, 1, 1, src_length)\n",
        "        return src_mask.to(self.device)\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        N, trg_len = trg.shape\n",
        "        trg_mask = torch.tril(torch.ones((trg_len,trg_len))).expand(N, 1,trg_len, trg_len)\n",
        "        return trg_mask.to(self.device)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
        "        return out"
      ],
      "metadata": {
        "id": "72eCRmC1mKql"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer: Full Model\n",
        "\n",
        "The `Transformer` class combines the **Encoder** and **Decoder** components, managing masks and facilitating end-to-end forward propagation.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Components\n",
        "\n",
        "1. **Encoder**:\n",
        "   - Processes the source sequence, mapping it into a context-rich latent representation.\n",
        "   - Parameters:\n",
        "     - `src_vocab_size`: Vocabulary size for the source language.\n",
        "     - `embed_size`: Dimensionality of embeddings.\n",
        "     - `num_layers`: Number of stacked Transformer blocks.\n",
        "     - `heads`: Number of attention heads.\n",
        "     - `forward_expansion`: Expansion factor for feed-forward layers.\n",
        "     - `dropout`: Dropout rate for regularization.\n",
        "     - `max_length`: Maximum input sequence length.\n",
        "\n",
        "2. **Decoder**:\n",
        "   - Generates the output sequence using the encoded representation and target sequence.\n",
        "   - Parameters mirror the encoder, except the target vocabulary size (`trg_vocab_size`).\n",
        "\n",
        "3. **Masks**:\n",
        "   - **Source Mask (`src_mask`)**:\n",
        "     - Ensures that padding tokens in the source sequence are ignored during attention.\n",
        "   - **Target Mask (`trg_mask`)**:\n",
        "     - Prevents the decoder from attending to future tokens during training, preserving causality.\n",
        "\n",
        "4. **Device Management**:\n",
        "   - Ensures all computations occur on the specified device (`cpu` or `gpu`).\n",
        "\n",
        "---\n",
        "\n",
        "## Code Walkthrough\n",
        "\n",
        "### Initialization (`__init__`)\n",
        "1. **Encoder and Decoder Initialization**:\n",
        "   - Creates encoder and decoder with matching embedding sizes, layer counts, and attention heads.\n",
        "   \n",
        "2. **Padding Indices**:\n",
        "   - `src_pad_idx`: Index of padding token in the source vocabulary.\n",
        "   - `trg_pad_idx`: Index of padding token in the target vocabulary.\n",
        "\n",
        "3. **Device Assignment**:\n",
        "   - The model is designed to run on the specified computation device.\n",
        "\n",
        "---\n",
        "\n",
        "### Mask Creation Functions\n",
        "#### `make_src_mask(src)`\n",
        "- Purpose: Masks out padding tokens in the source sequence.\n",
        "\\[\n",
        "\\text{src\\_mask} = (\\text{src} \\neq \\text{src\\_pad\\_idx}).\\text{unsqueeze}(1).\\text{unsqueeze}(2)\n",
        "\\]\n",
        "- Shape:\n",
        "  \\[\n",
        "  (N, 1, 1, \\text{src\\_len})\n",
        "  \\]\n",
        "\n",
        "#### `make_trg_mask(trg)`\n",
        "- Purpose: Masks out future tokens in the target sequence.\n",
        "- Creates a lower triangular matrix:\n",
        "\\[\n",
        "\\text{trg\\_mask} = \\text{torch.tril}(\\text{torch.ones}((\\text{trg\\_len}, \\text{trg\\_len})))\n",
        "\\]\n",
        "- Expanded to match batch size:\n",
        "\\[\n",
        "\\text{trg\\_mask}.\\text{expand}(N, 1, \\text{trg\\_len}, \\text{trg\\_len})\n",
        "\\]\n",
        "- Shape:\n",
        "  \\[\n",
        "  (N, 1, \\text{trg\\_len}, \\text{trg\\_len})\n",
        "  \\]\n",
        "\n",
        "---\n",
        "\n",
        "### Forward Pass\n",
        "#### Inputs:\n",
        "1. `src`: Source sequence of shape \\((N, \\text{src\\_len})\\).\n",
        "2. `trg`: Target sequence of shape \\((N, \\text{trg\\_len})\\).\n",
        "\n",
        "#### Steps:\n",
        "1. **Source Mask**:\n",
        "   \\[\n",
        "   \\text{src\\_mask} = \\text{make\\_src\\_mask(src)}\n",
        "   \\]\n",
        "\n",
        "2. **Target Mask**:\n",
        "   \\[\n",
        "   \\text{trg\\_mask} = \\text{make\\_trg\\_mask(trg)}\n",
        "   \\]\n",
        "\n",
        "3. **Encoder**:\n",
        "   - Processes the source sequence:\n",
        "   \\[\n",
        "   \\text{enc\\_src} = \\text{encoder(src, src\\_mask)}\n",
        "   \\]\n",
        "   - Shape of `enc_src`: \\((N, \\text{src\\_len}, \\text{embed\\_size})\\).\n",
        "\n",
        "4. **Decoder**:\n",
        "   - Combines target sequence with encoder output:\n",
        "   \\[\n",
        "   \\text{out} = \\text{decoder(trg, enc\\_src, src\\_mask, trg\\_mask)}\n",
        "   \\]\n",
        "   - Shape of `out`: \\((N, \\text{trg\\_len}, \\text{trg\\_vocab\\_size})\\).\n",
        "\n",
        "---\n",
        "\n",
        "### Output:\n",
        "- `out`: Predictions for each token in the target sequence.\n",
        "- Shape: \\((N, \\text{trg\\_len}, \\text{trg\\_vocab\\_size})\\).\n",
        "\n",
        "---\n",
        "\n",
        "## Summary of Functionality:\n",
        "1. **Encoder**:\n",
        "   - Maps the source sequence to a contextual representation.\n",
        "\n",
        "2. **Masks**:\n",
        "   - Handle padding and causality to ensure proper attention.\n",
        "\n",
        "3. **Decoder**:\n",
        "   - Incorporates encoder context and generates output predictions.\n",
        "\n",
        "This implementation provides the framework for sequence-to-sequence tasks like translation, summarization, and text generation.\n"
      ],
      "metadata": {
        "id": "S885nihgmVQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    device = torch.device(\"cpu\")\n",
        "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    x = torch.tensor([[1,5,6,4,3,9,5,2,0],[1,8,7,3,4,5,6,7,2]]).to(device)\n",
        "    trg = torch.tensor([[1,7,4,3,5,9,2,0],[1,5,6,2,4,7,6,2]]).to(device)\n",
        "\n",
        "    src_pad_idx, trg_pad_idx = 0,0\n",
        "    src_vocab_size, trg_vocab_size = 10,10\n",
        "    model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx).to(device)\n",
        "\n",
        "    out = model(x, trg[:,:-1])\n",
        "    print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3jr1HXkmZc9",
        "outputId": "2fee77a7-38a8-43d3-dbf3-c39589e49273"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 7, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lFkbeZxQmY5y"
      }
    }
  ]
}